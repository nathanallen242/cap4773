{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd2257a-f281-458b-a3d5-943d1fb605bd",
   "metadata": {},
   "source": [
    "### Data Pre-Processing & Visualization\n",
    "Given the size of the dataset (50 GB), we will need to stream its contents for each Q1-3.\n",
    "We will extract the following objects:\n",
    "* repositories\n",
    "* users\n",
    "\n",
    "Each object will have several associated features which will be determined as we conduct EDA (exploratory data analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cf3a1-76c6-479e-93c6-136d812e3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic utilities and data handling\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning preprocessing and modeling\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Data serialization\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd6e31",
   "metadata": {},
   "source": [
    "Loads primary dataset as JSON into a dataframe based on chunk size (e.g. 1000 JSON entries per chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca83d92-d6f3-4046-8a24-904989fcc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging purposes; loads & prints the first json entry only\n",
    "def load_json(file, index = 0):\n",
    "    try:\n",
    "        chunks = pd.read_json(file, lines=True, chunksize=1000)\n",
    "        \n",
    "        repo_list = None\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i == index:\n",
    "                repo_list = chunk[\"repo_list\"].iloc[0]\n",
    "                print(repo_list)\n",
    "                break\n",
    "        df = pd.DataFrame(repo_list)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(\"sample.csv\", index=False)\n",
    "        print(f\"Data saved to data.csv\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_path}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20103e-92e5-457a-9d35-ffb0a22ec0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data.json'\n",
    "load_json(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15715323-c4ef-4ee7-a171-88de5f952e06",
   "metadata": {},
   "source": [
    "### Features for Q1:\n",
    "* repo_list (to access the repo's and their nested features)\n",
    "* all repo features most associated with 'popularity'\n",
    "* id (user's id)\n",
    "* login (to identify the user; e.g. I would call Hashem drthetasigma on github, not Hashem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13248e-68bb-4853-879b-10fb8d1fb17c",
   "metadata": {},
   "source": [
    "### Preprocessing for ALL Repository Data, 1000 chunks/objects at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e982a6-4270-47d7-a109-7dd1beb0bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract specified features\n",
    "def preprocess_data(infile, outfile, chunk_size=1000):\n",
    "    first_chunk = True # header needs to be written to CSV for first write operation, after which it will be disabled\n",
    "\n",
    "    for chunk in pd.read_json(infile, lines=True, chunksize=chunk_size):\n",
    "        repos = []\n",
    "        for _, row in chunk.iterrows():\n",
    "            user_login = row['login']\n",
    "            repo_list = row['repo_list'] if row['repo_list'] else []\n",
    "\n",
    "            for repo in repo_list:\n",
    "                repo_data = {\n",
    "                    'user_login': user_login,\n",
    "                    'repo_full_name': repo.get('full_name', ''),\n",
    "                    'repo_id': repo.get('id', ''),\n",
    "                    'repo_description': repo.get('description', ''),\n",
    "                    'repo_size': repo.get('size', 0),\n",
    "                    'repo_license': repo.get('license', ''),\n",
    "                    'repo_stargazers_count': repo.get('stargazers_count', 0),\n",
    "                    'repo_fork': repo.get('fork', False),\n",
    "                    'repo_owner_id': repo.get('owner_id', ''),\n",
    "                    'repo_created_at': repo.get('created_at', ''),\n",
    "                    'repo_pushed_at': repo.get('pushed_at', ''),\n",
    "                    'repo_updated_at': repo.get('updated_at', ''),\n",
    "                    'repo_has_wiki': repo.get('has_wiki', False),\n",
    "                    'repo_open_issues': repo.get('open_issues', 0),\n",
    "                    'repo_language': repo.get('language', ''),\n",
    "                    'repo_forks_count': repo.get('forks_count', 0),\n",
    "                    'repo_default_branch': repo.get('default_branch', '')    \n",
    "                }\n",
    "                repos.append(repo_data)\n",
    "        \n",
    "        df_processed = pd.DataFrame(repos)\n",
    "        write_mode = 'w' if first_chunk else 'a'\n",
    "        df_processed.to_csv(outfile, mode=write_mode, index=False, header=first_chunk)\n",
    "        first_chunk = False\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9276f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data('data.json', './datasets/repos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4973ad-7fe8-440d-9e4b-3f706af4807b",
   "metadata": {},
   "source": [
    "### Relevant Features\n",
    "\n",
    "* Repo Name\n",
    "* Repo Size\n",
    "* Repo Star Count\n",
    "* Repo Fork Count\n",
    "* Repo Fork (1- True, 0 - False)\n",
    "* Repo Age (Use created_at and pushed_at/updated_at to create ths new feature)\n",
    "* Repo Open Issues\n",
    "* Repo Language\n",
    "\n",
    "\n",
    "#### But that leaves the question of, which metrics do we want to train our models to predict for 'popularity'?\n",
    "1. Star Count\n",
    "2. Fork Count\n",
    "3. Open Issue Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423ac47-1146-411e-878a-2e294bdbce95",
   "metadata": {},
   "source": [
    "### Let's extract the *important features* first!\n",
    "\n",
    "How:\n",
    "* Read 1000 rows of the CSV file per iteration.\n",
    "* Manually create 'age' feature from existing columns.\n",
    "* Append modified rows to chunk_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec29b1e-82f5-48b6-b3b6-316e6fe9b72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the columns you want to keep\n",
    "columns_to_keep = [\n",
    "    'repo_full_name', 'repo_size', 'repo_stargazers_count',\n",
    "    'repo_forks_count', 'repo_fork', 'repo_language',\n",
    "    'repo_created_at', 'repo_open_issues', 'repo_description',\n",
    "    'repo_updated_at'\n",
    "]\n",
    "\n",
    "# Define the chunk size\n",
    "chunksize = 10**3  # chunk_size = 1000\n",
    "\n",
    "# Initialize an empty list to store each chunk DataFrame\n",
    "chunk_list = []\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "for chunk in pd.read_csv('./datasets/repos.csv', usecols=columns_to_keep, chunksize=chunksize):\n",
    "\n",
    "    # Convert the 'repo_created_at' and 'repo_updated_at' columns to datetime\n",
    "    chunk['repo_created_at'] = pd.to_datetime(chunk['repo_created_at'])\n",
    "    chunk['repo_updated_at'] = pd.to_datetime(chunk['repo_updated_at'])\n",
    "\n",
    "    # Calculate the 'repo_age' in days as the difference between 'repo_created_at' and 'repo_updated_at'\n",
    "    chunk['repo_age'] = (chunk['repo_updated_at'] - chunk['repo_created_at']).dt.days\n",
    "\n",
    "    # Append the processed chunk to the list\n",
    "    chunk_list.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59b23a",
   "metadata": {},
   "source": [
    "How many values do we need to impute? How will we impute them? Let's observe the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a3d41-1b42-4ff8-b51b-302f7142b17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for null (NaN) values in the DataFrame\n",
    "null_values = df.isnull().sum()\n",
    "print(\"Null values in each column:\\n\", null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285799b-fba3-46fb-bbd8-b0cfd5783fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows/repos do we have?\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ba54f",
   "metadata": {},
   "source": [
    "## Correlation Matrices (Pearson, Spearman, Kendall) on Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab87cd-1151-4975-bcee-564c5f4a0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target features\n",
    "target_features = df[['repo_stargazers_count',\n",
    "                      'repo_forks_count',\n",
    "                      'repo_open_issues']]\n",
    "\n",
    "# Calculate Pearson's correlation\n",
    "pearson_correlation = target_features.corr(method='pearson')\n",
    "print(\"Pearson's correlation:\")\n",
    "print(pearson_correlation)\n",
    "print()\n",
    "\n",
    "# Calculate Spearman's rank correlation\n",
    "spearman_correlation = target_features.corr(method='spearman')\n",
    "print(\"Spearman's rank correlation:\")\n",
    "print(spearman_correlation)\n",
    "print()\n",
    "\n",
    "# Calculate Kendall's Tau correlation\n",
    "kendall_correlation = target_features.corr(method='kendall')\n",
    "print(\"Kendall's Tau correlation:\")\n",
    "print(kendall_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe156c8-b848-4669-8dfd-2bd450f31577",
   "metadata": {},
   "source": [
    "#### Impute missing values within columns: repository language and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5f018-83c3-45c3-a13a-50f1d473dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['repo_language'] = df['repo_language'].fillna('No language')\n",
    "df['repo_description'] = df['repo_description'].fillna('No description')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1e65",
   "metadata": {},
   "source": [
    "#### Save to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05fd6f3-564f-4aa0-a07c-eef6b3f63906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets/original_repos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61c814-5f23-485a-a09e-4795f12117dc",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "* Using only the top 9 languages by frequency, and grouping all other languages into the 'Other' category\n",
    "* Binary-encoded columns to represent the presence of a language (1 - in use, 0 - no use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to all zero values in 'repo_age'\n",
    "df['repo_age'] = df['repo_age'].apply(lambda x: x if x > 0 else x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'repo_fork' from boolean to integers (1 for True, 0 for False)\n",
    "df['repo_fork'] = df['repo_fork'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top 10 languages and grouping the rest as 'Other'\n",
    "top_n = 9\n",
    "top_languages = df['repo_language'].value_counts().nlargest(top_n).index\n",
    "df['repo_language'] = df['repo_language'].apply(lambda x: x if x in top_languages else 'Other')\n",
    "\n",
    "\n",
    "# One-hot encoding for 'repo_language'\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "language_encoded = ohe.fit_transform(df[['repo_language']])\n",
    "language_encoded_df = pd.DataFrame(language_encoded, columns=ohe.get_feature_names_out(['repo_language']))\n",
    "\n",
    "# Drop the original 'repo_language' column and concatenate the one-hot encoded language dataframe\n",
    "df_ohe = df.drop('repo_language', axis=1)\n",
    "df_ohe = pd.concat([df_ohe, language_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out zero values in repo_size, target features, etc.\n",
    "columns_to_check = ['repo_size', 'repo_age', 'repo_stargazers_count', 'repo_open_issues', 'repo_forks_count']\n",
    "\n",
    "# Filtering out rows where any specified column has value <= 0\n",
    "filtered_ohe = df.loc[(df_ohe[columns_to_check] > 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362f53e-5da5-457b-b618-04886c31131f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "numerical_columns = ['repo_size', 'repo_stargazers_count', 'repo_open_issues', 'repo_forks_count', 'repo_age']\n",
    "categorical_columns = ['repo_fork', 'repo_language']\n",
    "\n",
    "print(f\"Maximum repo size: {df['repo_size'].max()}\")\n",
    "print(f\"Maximum Star Count: {df['repo_stargazers_count'].max()}\")\n",
    "print(f\"Maximum Issue Count: {df['repo_open_issues'].max()}\")\n",
    "\n",
    "\n",
    "# Plotting the numerical columns after applying log1p transformation\n",
    "for i, column in enumerate(numerical_columns, 1):\n",
    "    transformed_column = np.log1p(filtered_df[column])  # Applying log1p transformation\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(transformed_column, kde=False, bins=30)\n",
    "    plt.title(f'Log-transformed Distribution of {column}')\n",
    "    \n",
    "# Since repo_full_name and repo_language could have many unique values, we plot only the top 10 for demonstration\n",
    "# Adjust the number of shown categories based on your data and needs\n",
    "for i, column in enumerate(categorical_columns, len(numerical_columns) + 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    top_categories = df[column].value_counts().head(10)\n",
    "    sns.barplot(x=top_categories.index, y=top_categories)\n",
    "    plt.title(f'Top categories of {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe56392-c1f5-4932-a598-c602a7df1383",
   "metadata": {},
   "source": [
    "#### Top Repositories by Star Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f6013-f6a3-4d81-a441-a407ddba9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 number of stars\n",
    "df.sort_values(by='repo_stargazers_count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290c035-11fb-4645-898a-fec8663a0883",
   "metadata": {},
   "source": [
    "#### Top Repositories by Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d50d628-708c-48d3-ac24-fe7bb9377e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 number of size\n",
    "df.sort_values(by='repo_size', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece4bc5-3a7f-4e43-9204-90f90503cddb",
   "metadata": {},
   "source": [
    "#### Top Repositories by Issue Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aba72-d0ea-4636-bc34-7cdf7e83f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 number of issue counts\n",
    "df.sort_values(by='repo_open_issues', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000af11-8f95-428a-8e29-f3f2fe5c690f",
   "metadata": {},
   "source": [
    "#### Top Repositories by Fork Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690c2163-d2a1-47fe-8b53-9292a89c2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 number of issue counts\n",
    "df.sort_values(by='repo_forks_count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba0e0e-cbf0-485e-badc-28d69bc77301",
   "metadata": {},
   "source": [
    "#### Percentage Distribution of Forked Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4145f27-eaf1-4a3c-a0b6-28d101531645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage distribution\n",
    "percentage_distribution = df['repo_fork'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Displaying the percentage distribution\n",
    "print(percentage_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e672e-ea8c-485e-99ea-54b70292bab9",
   "metadata": {},
   "source": [
    "##### Before we decide to keep/drop this column, let's observe the characteristics of forked vs unforked repos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb8681-1488-4352-9d19-a610fc2185a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forked_repos = df[df['repo_fork'] == True]\n",
    "non_forked_repos = df[df['repo_fork'] == False]\n",
    "\n",
    "print(\"Forked Repositories:\")\n",
    "print(forked_repos.describe())\n",
    "\n",
    "print(\"\\nNon-Forked Repositories:\")\n",
    "print(non_forked_repos.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374dbf76-7b47-4627-b6d4-bccefffd6a79",
   "metadata": {},
   "source": [
    "#### Distribution of Star Count (Forked vs Non-Forked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8af312-53c9-4a43-930f-cafc06479389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing the distribution of stargazers count\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(forked_repos['repo_stargazers_count'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Forked Repos - Star Count')\n",
    "plt.xlabel('Star Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(non_forked_repos['repo_stargazers_count'], bins=50, color='green', alpha=0.7)\n",
    "plt.title('Non-Forked Repos - Star Count')\n",
    "plt.xlabel('Star Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032833a6-5284-4018-9677-4d4bd6ab184f",
   "metadata": {},
   "source": [
    "#### Displaying all Languages Used:\n",
    "* Langages below 2% will be grouped into Other for visualization purposes.\n",
    "* Languages within the 'Other' group will be displayed in a separate graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ddebf-8274-4d77-81fb-565c8e7a7de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each language and calculate the percentage\n",
    "language_counts = df['repo_language'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Identify languages to be grouped into 'Other'\n",
    "others = language_counts[language_counts <= 2]\n",
    "# Sum their percentages\n",
    "others_percentage = others.sum()\n",
    "\n",
    "# Filter out the 'others'\n",
    "main_languages = language_counts[language_counts > 2]\n",
    "\n",
    "# If there are any languages in 'others', add them as a single entry\n",
    "if len(others) > 0:\n",
    "    main_languages['Other'] = others_percentage\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(main_languages, labels=main_languages.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.axis('equal')  # Ensure pie chart is circular\n",
    "plt.title('Distribution of Repository Languages')\n",
    "plt.show()\n",
    "\n",
    "# Convert 'others' Series to DataFrame for a nicer display\n",
    "others_df = others.reset_index()\n",
    "others_df.columns = ['Language', 'Percentage']\n",
    "\n",
    "# Display the DataFrame\n",
    "display(others_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f77b6-85c5-42f5-89e3-54db6af3d09c",
   "metadata": {},
   "source": [
    "### Feature Selection: One-Hot Encoding + LASSO Regression\n",
    "\n",
    "* **Why One-Hot Encoding?** Converts categorical variables into a form that could be provided to ML algorithms to do a better job in prediction by creating dummy variables that indicate the presence of an attribute.\n",
    "* **Why LASSO Regression?** Employs shrinkage where data values are shrunk towards a central point as the mean. This technique helps in feature selection by reducing the coefficients of less important features to zero, effectively removing them from the equation.\n",
    "* This method assists in preserving important features and minimizing/discarding unnecessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac85964-2690-40ae-80da-ef89418155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000  # You can adjust this size depending on your system's memory capacity\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('./datasets/original_repos.csv', chunksize=chunk_size):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Concatenate chunks to form the full DataFrame\n",
    "df_orig = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c7df2-d231-416e-8d26-03add2185772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'repo_fork' from boolean to integers (1 for True, 0 for False)\n",
    "df_orig['repo_fork'] = df_orig['repo_fork'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6720f677-6e0f-4177-8002-163fdd24d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the top 10 languages and grouping the rest as 'Other'\n",
    "top_n = 9\n",
    "top_languages = df_orig['repo_language'].value_counts().nlargest(top_n).index\n",
    "df_orig['repo_language'] = df_orig['repo_language'].apply(lambda x: x if x in top_languages else 'Other')\n",
    "\n",
    "# Creating a copy for the non-one-hot encoded version\n",
    "df_non_ohe = df_orig.copy()\n",
    "\n",
    "# One-hot encoding for 'repo_language'\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "language_encoded = ohe.fit_transform(df_orig[['repo_language']])\n",
    "language_encoded_df = pd.DataFrame(language_encoded, columns=ohe.get_feature_names_out(['repo_language']))\n",
    "\n",
    "# Drop the original 'repo_language' column and concatenate the one-hot encoded language dataframe\n",
    "df_ohe = df_orig.drop('repo_language', axis=1)\n",
    "df_ohe = pd.concat([df_ohe, language_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2fbdc-053d-4f9e-a00a-e2147480c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbefe7",
   "metadata": {},
   "source": [
    "### Feature Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbbb301-77eb-4185-995b-361821162da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, target_name, model_params=None, classification=False):\n",
    "    if model_params is None:\n",
    "        model_params = {'n_estimators': 50, 'random_state': 42} # n_estimators = 100 for other instances, set to 50 for target encoding\n",
    "    if classification:\n",
    "        model = RandomForestClassifier(**model_params)\n",
    "    else:\n",
    "        model = RandomForestRegressor(**model_params)\n",
    "    model.fit(X, y)\n",
    "    feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    print(f\"Feature importances for predicting {target_name}:\\n{feature_importances}\\n\")\n",
    "    feature_importances.plot(kind='bar')\n",
    "    plt.title(f\"Feature Importances for {target_name}\")\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.show()\n",
    "    selector = SelectFromModel(model, prefit=True)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    print(f\"Selected features for predicting {target_name}:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a046a-5168-480f-a24f-55798c337c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original DataFrame and df_ohe already contains one-hot encoded features\n",
    "normalized_features = ['repo_stargazers_count', 'repo_open_issues', 'repo_forks_count']\n",
    "\n",
    "# Filter out rowas where repo_age = 0\n",
    "df_orig = df_orig[df_orig['repo_age'] != 0]\n",
    "\n",
    "for feature in normalized_features:\n",
    "    df_ohe[f\"{feature}_per_year\"] = df_orig[feature] / df_orig['repo_age']\n",
    "\n",
    "# Presuming you want to drop certain non-target, non-feature columns like 'repo_full_name', 'repo_description'\n",
    "columns_to_drop = ['repo_full_name', 'repo_description'] + normalized_features\n",
    "df_modified = df_ohe.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "normalized_targets = [f\"{feature}_per_year\" for feature in normalized_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25957b-bf01-4997-8823-750170286b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = df_ohe[df_ohe['repo_age'] != 0]\n",
    "df_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ef6fe-e8ca-4a09-a95d-e77def81d9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for target in normalized_targets:\n",
    "    print(f\"Feature selection for: {target}\")\n",
    "    # Ensure 'repo_age' is included as a feature, exclude all other normalized targets except the current one\n",
    "    X_columns = [col for col in df_modified.columns if col not in normalized_targets or col == target]\n",
    "    X_columns.remove(target)  # Remove the current target from the feature set\n",
    "    print(X_columns)\n",
    "    \n",
    "    X = df_modified[X_columns]\n",
    "    y = df_ohe[target].dropna() # Accessing from df_ohe to ensure it's always available\n",
    "    feature_selection(X, y, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea801f0",
   "metadata": {},
   "source": [
    "## LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39af8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe for missing columns, before beginning\n",
    "df_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_modified is your DataFrame and normalized_targets are your targets\n",
    "normalized_features = ['repo_stargazers_count', 'repo_open_issues', 'repo_forks_count']\n",
    "normalized_targets = [f\"{feature}_per_year\" for feature in normalized_features]\n",
    "X = df_modified.drop(normalized_targets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3129a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modified.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c876553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing & debugging\n",
    "df_modified[normalized_targets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4804716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Scale X outside the loop\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dictionary to store optimal alphas and selected strong features\n",
    "optimal_alphas = {}\n",
    "selected_features_per_target = {}\n",
    "\n",
    "for normalized_target in normalized_targets:\n",
    "    y = df_modified[normalized_target]\n",
    "\n",
    "    # 2. Split the data into training and testing sets\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 3. LassoCV for automatic alpha selection, using 5-fold cross-validation\n",
    "    lasso_cv = LassoCV(cv=5, random_state=42, max_iter=10000)\n",
    "    lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # After fitting LassoCV\n",
    "    cv_means = lasso_cv.mse_path_.mean(axis=1)\n",
    "    cv_stds = lasso_cv.mse_path_.std(axis=1)\n",
    "    min_error = np.min(cv_means)\n",
    "    min_error_std = cv_stds[np.argmin(cv_means)]\n",
    "\n",
    "    # Select the largest alpha with mean error within one std of the minimum\n",
    "    eligible_alphas = lasso_cv.alphas_[cv_means <= min_error + min_error_std]\n",
    "    target_alpha = eligible_alphas[-1]  # The largest alpha meeting the criteria\n",
    "\n",
    "    optimal_alphas[normalized_target] = target_alpha\n",
    "\n",
    "    # Fit Lasso model with the selected best alpha on the training data\n",
    "    lasso = Lasso(alpha=target_alpha, max_iter=10000, random_state=42)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # 4. Determine a threshold for selecting features, e.g., the 75th percentile of the absolute coefficients\n",
    "    coefs = lasso.coef_\n",
    "    threshold = np.percentile(np.abs(coefs), 75)  # Adjust the percentile as needed\n",
    "\n",
    "    # Select features where the absolute coefficient is above the threshold\n",
    "    strong_features = X.columns[np.abs(coefs) > threshold].tolist()\n",
    "\n",
    "    # Store the selected strong features\n",
    "    selected_features_per_target[normalized_target] = strong_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b418f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimal alphas and selected strong features for each target\n",
    "print(\"Optimal Alphas:\")\n",
    "for target, alpha in optimal_alphas.items():\n",
    "    print(f\"Optimal alpha for {target}: {alpha}\")\n",
    "\n",
    "print(\"\\nSelected Strong Features:\")\n",
    "for target, features in selected_features_per_target.items():\n",
    "    print(f\"Selected strong features for {target}: {features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
